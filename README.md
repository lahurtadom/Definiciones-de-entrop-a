# Definiciones de entropia
## 1. Entropía en Termodinámica
La entropía es una magnitud física que mide el grado de desorden o dispersión de energía en un sistema.  
Se relaciona con el Segundo Principio de la Termodinámica: en un proceso natural, la entropía total del universo tiende a aumentar.

**Fórmula básica:**
dS = dQ_rev / T

---

## 2. Entropía Estadística (Boltzmann)
La entropía representa el número de microestados posibles de un sistema.  
Mientras más microestados posibles existan, mayor será la entropía.

**Fórmula de Boltzmann:**
S = k · ln(Ω)

---

## 3. Entropía de Shannon (Teoría de la Información)
En la teoría de la información, la entropía mide la incertidumbre en un conjunto de mensajes o datos.  
Más incertidumbre = mayor entropía.

**Fórmula:**
H(X) = – Σ p(xi) · log₂ p(xi)

---

## 4. Entropía Cuántica (von Neumann)
Es una medida del desorden o la falta de información en un sistema cuántico.  
Se usa para analizar el entrelazamiento cuántico.

**Fórmula:**
S = – Tr(ρ log ρ)

---

## 5. Entropía en Ecología
Mide la diversidad de especies en un ecosistema.  
Se suele usar la entropía de Shannon para medir cuán distribuida está la abundancia entre las especies.

---

## 6. Idea General
Aunque aparece en muchos campos, la entropía siempre tiene que ver con:
- Desorden  
- Incertidumbre  
- Información  
- Número de configuraciones posibles

Es una medida del “caos” o la impredecibilidad de un sistema.
